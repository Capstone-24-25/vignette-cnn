{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling Notebook**\n",
    "\n",
    "- This Notebook will be used for testing different models which will then be included in the final Vignette report.\n",
    "\n",
    "We will be testing the following models:\n",
    "- **Simple CNN**\n",
    "    - 3 Convolution Layers with 64 filters for the first 2 and 128 for the second,\n",
    "    - Maxpooling\n",
    "    - and a Dense Layer with 128 nodes with a dropout of 50%\n",
    "- **Hyperband Parameter Training**\n",
    "    - Uses Keras Hyperband Tuning to start with different architectures at lower epochs and slowly removes unpromising setups, and keeps the best ones and trains further\n",
    "- **Bayesian Optimization**\n",
    "    - Uses Optuna to efficiently search the hyperparameter space by modeling the objective function and selecting hyperparameters that are expected to improve the model's performance. This approach is particularly useful for optimizing complex models with many hyperparameters.\n",
    "- Pre-trained Model additional layering\n",
    "    - on top of convolution layers to improve \n",
    "    - by how much does it improve\n",
    "\n",
    "Considerations:\n",
    "- Do we need to consider a better data set?\n",
    "- how can we get better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### library imports ##### \n",
    "\n",
    "# preprocessing and splitting data packages\n",
    "import os\n",
    "from Preprocessing import preprocess_data\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# modeling data packages\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "import optuna # for bayesian optimization\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data from Preprocessing file\n",
    "\n",
    "- Pulls 400 Brain Tumor MRI images from each of 4 different classes, including:\n",
    "    - Glioma\n",
    "    - Meningioma  \n",
    "    - Pituitary\n",
    "    - Normal (no tumor)\n",
    "\n",
    "- Ensures they are normalizes, resized, and Grayscaled for quicker computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/reese/Documents/School/UC Santa Barbra/PSTAT/PSTAT197/vignette-cnn/scripts/drafts\n",
      "processed glioma data\n",
      "(400, 64, 64, 1)\n",
      "processed meningioma data\n",
      "(400, 64, 64, 1)\n",
      "processed pituitary data\n",
      "(399, 64, 64, 1)\n",
      "processed normal data\n",
      "(400, 64, 64, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All data must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(normal_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m glioma_data\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m meningioma_data\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mor\u001b[39;00m pituitary_data\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m normal_data\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll data must have the same shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll data have the same shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: All data must have the same shape"
     ]
    }
   ],
   "source": [
    "my_dir = os.getcwd()\n",
    "print(f\"Current working directory: {my_dir}\")\n",
    "\n",
    "# change this accordingly but should work for all users\n",
    "glioma_path = my_dir[:-15] + \"/data/glioma_tumor\"\n",
    "meningioma_path = my_dir[:-15] + \"/data/meningioma_tumor\"\n",
    "pituitary_path = my_dir[:-15] + \"/data/pituitary_tumor\"\n",
    "normal_path = my_dir[:-15] + \"/data/normal_tumor\"\n",
    "\n",
    "\n",
    "# preprocess data\n",
    "n_samples = 400 # good starting point to train model\n",
    "seed = 1234 # reproducibility\n",
    "glioma_data, glioma_labels = preprocess_data(glioma_path, n_samples, seed)\n",
    "print(\"processed glioma data\")\n",
    "print(glioma_data.shape)\n",
    "meningioma_data, meningioma_labels = preprocess_data(meningioma_path, n_samples, seed)\n",
    "print(\"processed meningioma data\")\n",
    "print(meningioma_data.shape)\n",
    "pituitary_data, pituitary_labels = preprocess_data(pituitary_path, n_samples, seed)\n",
    "print(\"processed pituitary data\")\n",
    "print(pituitary_data.shape)\n",
    "normal_data, normal_labels = preprocess_data(normal_path, n_samples, seed)\n",
    "print(\"processed normal data\")\n",
    "print(normal_data.shape)\n",
    "if glioma_data.shape != meningioma_data.shape or pituitary_data.shape != normal_data.shape:\n",
    "    raise ValueError(\"All data must have the same shape\")\n",
    "else:\n",
    "    print(\"All data have the same shape\")\n",
    "\n",
    "# combine all data and labels\n",
    "data = np.concatenate([glioma_data, meningioma_data, pituitary_data, normal_data])\n",
    "labels = np.concatenate([glioma_labels, meningioma_labels, pituitary_labels, normal_labels])\n",
    "print(f\"Combined data and labels shape: {data.shape}, {labels.shape}\")\n",
    "\n",
    "# split data into training and testing and validation sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "train_small, val_data, train_small_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# one hot encode labels\n",
    "label_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels = label_encoder.fit_transform(np.array(train_labels).reshape(-1, 1))\n",
    "test_labels = label_encoder.transform(np.array(test_labels).reshape(-1, 1))\n",
    "train_small_labels = label_encoder.transform(np.array(train_small_labels).reshape(-1, 1))\n",
    "val_labels = label_encoder.transform(np.array(val_labels).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visually Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform labels\n",
    "labels = label_encoder.inverse_transform(train_labels)\n",
    "\n",
    "# show images from training set\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(train_data[i], cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.title((labels[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "- ### We first can start with a simple convolution model as a base level to see how well one archictecture will learn on the training data set.\n",
    "- ### Next, we will apply a hyperparameter tuning technique, called Hyperband tuning, to find the best setup.\n",
    "     - Hyperband tuning performs a combination of random search and early stopping to efficiently explore hyperparameter configurations. It allocates resources to promising configurations while quickly discarding those that are underperforming, allowing for faster convergence to optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN to Start\n",
    "\n",
    "### What is a CNN?\n",
    "A CNN (convolutional neural network) is a type of network that specializes in processing and predicting grid-like data, making it especially useful for image classification. It utilizes automatic detection of spatial hierarchies and patterns with layers of convolutional filters. Unlike the classic feed-forward network, which connects each neuron to every neuron in the next layer, a CNN uses local receptive fields and shared weights in its convolutional layers, which reduces the number of parameters significantly, and improves the efficiency of the model. \n",
    "\n",
    "### Model Flow\n",
    "- In our example, the filters will pickup on different shapes, edges, and other patterns in the image, by sliding multiple 3x3 kernels convolving with the input image.\n",
    "- After this we, apply a pooling function of either averaging or maximizing which typically halves the size of our convolved feature.\n",
    "- Then after another round of convolving and maxpool, we flatten it and pass to a Dense neural network of hidden layers\n",
    "- Finally the output layer is activated by a softmax function to provide us with probabilities of the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Sequential([\n",
    "  Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(64, 64, 1)),\n",
    "  MaxPooling2D(2), # reduce dimensionality\n",
    "  Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "  MaxPooling2D(2), # reduce dimensionality\n",
    "  Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "  MaxPooling2D(2), # reduce dimensionality\n",
    "  Flatten(), # flatten the data to feed into the dense layers\n",
    "  Dense(units=128, activation='relu'),\n",
    "  Dropout(0.5), # prevent overfitting\n",
    "  Dense(units=4, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model # \n",
    "\n",
    "# Early stopping will stop training when the validation loss stops improving for a few epochs, preventing overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_split=0.2, # Convert sparse matrix\n",
    "    epochs=20,\n",
    "    batch_size=32, # mini batch size\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate model on test set\n",
    "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO LATER once decent performance on simple CNN:\n",
    "HyperParameter Tuning\n",
    "- We will apply Hyperband Parameter Tuning to find our best set of params\n",
    "- Then once we have the best set of hyperparams, we will build a final model and train with those parameters on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a hyperband model to find the best model architecture\n",
    "def build_model(hp):\n",
    "    '''Hyperband function to test different model architectures'''\n",
    "    # initialize the model\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
    "               kernel_size=hp.Choice('conv_1_kernel', values=[3,5]),\n",
    "               activation='relu', input_shape=(64, 64, 1)),\n",
    "        MaxPooling2D(2),\n",
    "        Conv2D(filters=hp.Int('conv_2_filter', min_value=32, max_value=128, step=16),\n",
    "               kernel_size=hp.Choice('conv_2_kernel', values=[3,5]),\n",
    "               activation='relu'),\n",
    "        MaxPooling2D(2),\n",
    "        Conv2D(filters=hp.Int('conv_3_filter', min_value=32, max_value=128, step=16),\n",
    "               kernel_size=3,\n",
    "               activation='relu'),\n",
    "        MaxPooling2D(2),\n",
    "        Flatten(),\n",
    "        Dense(units=hp.Int('dense_1_units', min_value=64, max_value=256, step=32),\n",
    "              activation='relu'),\n",
    "        Dropout(rate=hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "# initialize tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=30,\n",
    "    factor=5,\n",
    "    directory=my_dir[:-15] + \"/scripts/drafts/model_trials\",  # Use temporary directory\n",
    "    project_name='hp_tuning',\n",
    "    overwrite=False\n",
    ")\n",
    "# search for the best hyperparameters\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "tuner.search(\n",
    "    train_small, train_small_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_data, val_labels),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "# Get the best hyperparameters directly\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hp.values)\n",
    "# build the best model\n",
    "final_model = tuner.hypermodel.build(best_hp)\n",
    "final_model.compile(optimizer=keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our best model, we can then build using the best architecture and train on the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "history = final_model.fit(train_data, train_labels, \n",
    "                epochs=50,\n",
    "                validation_split=0.15,  \n",
    "                batch_size=32,\n",
    "                callbacks=[early_stopping])\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "test_loss, test_accuracy = final_model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # define hyperparameters to tune\n",
    "    filters = [trial.suggest_int(f'filters_layer_{i+1}', 32, 128, step=16) for i in range(3)]\n",
    "    kernel_size = trial.suggest_int('kernel_size', 3, 5)\n",
    "    dense_units = trial.suggest_int('dense_units', 64, 256, step=32)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "    # init model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=filters[0], \n",
    "                     kernel_size=kernel_size, \n",
    "                     activation='relu', input_shape=(64, 64, 1))) # specify input for first layer\n",
    "    model.add(MaxPooling2D(2))\n",
    "    for i in range(1, 3):\n",
    "        model.add(Conv2D(filters=filters[i], kernel_size=kernel_size, activation='relu'))\n",
    "        model.add(MaxPooling2D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4, activation='softmax')) # 4 classes\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # train the model\n",
    "    model.fit(train_small, train_small_labels, \n",
    "              epochs=20, # fewer epochs for faster training\n",
    "              validation_data=(val_data, val_labels), \n",
    "              callbacks=[early_stopping],\n",
    "              verbose=0)\n",
    "    # return the objective: accuracy\n",
    "    return model.evaluate(test_data, test_labels, verbose=1)[1] \n",
    "\n",
    "# Initialize study\n",
    "Bayes_study = optuna.create_study(direction='maximize')\n",
    "Bayes_study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters and the best accuracy\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(Bayes_study.best_params)\n",
    "print(\"Best Accuracy:\")\n",
    "print(Bayes_study.best_value)\n",
    "\n",
    "# Extract data from the study\n",
    "trials = Bayes_study.trials\n",
    "values = [trial.value for trial in trials]\n",
    "trial_numbers = [trial.number for trial in trials]\n",
    "\n",
    "# Plot optimization history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trial_numbers, values, marker='o')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Optimization History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build best bayesian model\n",
    "\n",
    "best_bayes_model = Bayes_study.best_params\n",
    "bayes_hp = kt.HyperParameters()\n",
    "\n",
    "for key, value in best_bayes_model.items(): # iterate through the best hyperparameters and set them\n",
    "    bayes_hp.Fixed(key, value)\n",
    "\n",
    "bayes_model = tuner.hypermodel.build(bayes_hp)\n",
    "bayes_model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_bayes_model['learning_rate']), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train best bayesian model\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True) # callback to prevent overfitting\n",
    "bayes_history = bayes_model.fit(train_data, train_labels, \n",
    "                          epochs=50, validation_split=0.2, \n",
    "                          batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# evaluate best bayesian model\n",
    "bayes_test_loss, bayes_test_accuracy = bayes_model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Loss: {bayes_test_loss}\")\n",
    "print(f\"Test Accuracy: {bayes_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Bayesian versus Hyperband Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy and loss of the two models and the test accuracy of the two models\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "ax[0].plot(history.history['accuracy'], label='HB Train Accuracy')\n",
    "ax[0].plot(history.history['val_accuracy'], label='HB Validation Accuracy')\n",
    "ax[0].plot(bayes_history.history['accuracy'], label='Bayes Train Accuracy')\n",
    "ax[0].plot(bayes_history.history['val_accuracy'], label='Bayes Validation Accuracy')\n",
    "ax[0].set_title('Model Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend()\n",
    "\n",
    "bars = ax[1].bar(['Bayes', 'Hyperband'], \n",
    "                 [bayes_test_accuracy, test_accuracy], \n",
    "                 color = ['blue', 'green'], \n",
    "                 label = ['Bayes', 'Hyperband'])\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax[1].text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom')\n",
    "ax[1].set_title('Test Accuracy')\n",
    "ax[1].set_xlabel('Model')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
